{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = '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'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts.prompt import PromptTemplate\n",
    "\n",
    "from config import Config\n",
    "from document import DocumentService\n",
    "from llm import LLMService\n",
    "\n",
    "\n",
    "class LangChainApplication(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.config = Config\n",
    "        self.llm_service = LLMService()\n",
    "        ###加载llm和知识库向量\n",
    "        print(\"load llm model \")\n",
    "        self.llm_service.load_model(model_name_or_path=self.config.llm_model_name)\n",
    "        self.doc_service = DocumentService()\n",
    "        print(\"load documents\")\n",
    "        self.doc_service.load_vector_store()\n",
    "\n",
    "    def get_knowledge_based_answer(self, query,\n",
    "                                   history_len=5,\n",
    "                                   temperature=0.1,\n",
    "                                   top_p=0.9,\n",
    "                                   top_k=1,\n",
    "                                   chat_history=[]):\n",
    "        #定义prompt\n",
    "        prompt_template = \"\"\"基于以下已知信息，简洁和专业的来回答用户的问题。\n",
    "                                        如果无法从中得到答案，请说 \"根据已知信息无法回答该问题\" 或 \"没有提供足够的相关信息\"，不允许在答案中添加编造成分，答案请使用中文。\n",
    "                                        已知内容:\n",
    "                                        {context}\n",
    "                                        问题:\n",
    "                                        {question}\"\"\"\n",
    "        prompt = PromptTemplate(template=prompt_template,\n",
    "                                input_variables=[\"context\", \"question\"])\n",
    "        self.llm_service.history = chat_history[-history_len:] if history_len > 0 else []\n",
    "\n",
    "        self.llm_service.temperature = temperature\n",
    "        self.llm_service.top_p = top_p\n",
    "        # 声明一个知识库问答llm,传入之前初始化好的llm和向量知识搜索服务\n",
    "        knowledge_chain = RetrievalQA.from_llm(\n",
    "            llm=self.llm_service,\n",
    "            retriever=self.doc_service.vector_store.as_retriever(\n",
    "                search_kwargs={\"k\": top_k}),\n",
    "            prompt=prompt)\n",
    "\n",
    "        knowledge_chain.combine_documents_chain.document_prompt = PromptTemplate(\n",
    "            input_variables=[\"page_content\"], template=\"{page_content}\")\n",
    "        knowledge_chain.return_source_documents = True\n",
    "\n",
    "        ### 基于知识库的问答\n",
    "        result = knowledge_chain({\"query\": query})\n",
    "        return result\n",
    "\n",
    "    def get_llm_answer(self, query=''):\n",
    "        prompt_template = \"\"\"请回答下列问题:\n",
    "                            {}\"\"\".format(query)\n",
    "        ### 基于大模型的问答\n",
    "        result = self.llm_service._call(prompt_template)\n",
    "        return result\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    application = LangChainApplication()\n",
    "    print(\"大模型自己回答的结果\")\n",
    "    result = application.get_llm_answer('迪丽热巴的作品有什么')\n",
    "    print(result)\n",
    "    print(\"大模型+知识库后回答的结果\")\n",
    "    result = application.get_knowledge_based_answer('迪丽热巴的作品有什么')\n",
    "    print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TCMHealth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.17"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
